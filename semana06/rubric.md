# Milestone 1: Project Proposal Evaluation Rubric

**Course**: Data Analytics
**Universidad Cooperativa de Colombia**
**Instructor**: Julian Eduardo Garzon Giraldo, MsC

---

## Overview

| Criterion | Weight | Max Points |
|-----------|--------|------------|
| Dataset Selection | 20% | 5 |
| Problem Statement | 20% | 5 |
| Research Questions | 25% | 5 |
| Methodology Plan | 20% | 5 |
| Presentation Quality | 15% | 5 |
| **Total** | **100%** | **25** |

---

## Criterion 1: Dataset Selection (20%)

Evaluates the appropriateness, quality, and feasibility of the chosen dataset.

| Score | Description |
|-------|-------------|
| **5 - Excellent** | Dataset is from datos.gov.co, highly relevant to Colombian public policy or social issues. Contains 1000+ rows and 10+ diverse columns (mix of numeric and categorical). Data is recent (within 3 years) and well-documented. Clear rationale for selection provided. |
| **4 - Good** | Dataset meets minimum requirements (datos.gov.co, 1000+ rows, 8+ columns). Relevance is clear. Some documentation available. Minor limitations acknowledged. |
| **3 - Acceptable** | Dataset is from datos.gov.co but may be smaller than ideal or have limited variable diversity. Relevance is present but not strongly argued. Documentation is sparse. |
| **2 - Below Expectations** | Dataset is borderline acceptable (fewer than 1000 rows or limited columns). Relevance to analytics project is questionable. No acknowledgment of limitations. |
| **1 - Not Acceptable** | Dataset is not from datos.gov.co (without prior approval), too small for meaningful analysis, or inappropriate for the course objectives. |

### Key Questions to Ask
- Is the dataset from datos.gov.co?
- Are there enough observations for statistical validity?
- Is there a good mix of variable types for different analyses?
- Is the data recent and relevant?

---

## Criterion 2: Problem Statement (20%)

Evaluates the clarity, specificity, and real-world relevance of the problem being addressed.

| Score | Description |
|-------|-------------|
| **5 - Excellent** | Problem statement is crystal clear, specific, and compelling. Clearly identifies who cares about this problem and why. Directly tied to the dataset. Could realistically inform a decision or policy. |
| **4 - Good** | Problem statement is clear and specific. Stakeholders are identified. Connection to dataset is evident. Real-world application is plausible. |
| **3 - Acceptable** | Problem statement is understandable but could be more specific. Stakeholders mentioned but not well-defined. Connection to dataset requires some inference. |
| **2 - Below Expectations** | Problem statement is vague or overly broad. Uses generic language like "explore" or "study" without specifics. Unclear who would benefit from the analysis. |
| **1 - Not Acceptable** | No clear problem statement provided, or the stated problem cannot be addressed with the chosen dataset. |

### Key Questions to Ask
- Can I explain this problem in one sentence?
- Who specifically would use this analysis?
- What decision or action could this analysis inform?

---

## Criterion 3: Research Questions (25%)

Evaluates the quality, specificity, and answerability of the three required research questions.

| Score | Description |
|-------|-------------|
| **5 - Excellent** | All three questions are specific, measurable, and clearly answerable with EDA techniques. Questions show progressive complexity (descriptive to relational). Each question directly supports the problem statement. Questions avoid causation claims. |
| **4 - Good** | Three questions provided, all are specific and answerable. At least two questions show clear connection to the problem statement. Questions are appropriately scoped for EDA. |
| **3 - Acceptable** | Three questions provided but some lack specificity. Questions are generally answerable but may need refinement. Connection to problem statement is present but could be stronger. |
| **2 - Below Expectations** | Fewer than three questions, or questions are too vague to answer. Some questions may imply causation or require methods beyond EDA. Weak connection to problem statement. |
| **1 - Not Acceptable** | Questions are missing, unanswerable with the given data, or fundamentally misunderstand the scope of EDA. |

### What Makes a Strong Research Question

**Strong Example**:
> "Which Colombian departments have the highest rates of school dropout among students aged 15-18, and how has this changed from 2018 to 2022?"

**Weak Example**:
> "Why do students drop out of school?"

### Checklist for Each Question
- [ ] Specific (mentions variables, timeframes, populations)
- [ ] Measurable (can be answered with numbers or clear categories)
- [ ] Answerable with EDA (does not require experiments or surveys)
- [ ] Connected to the problem statement
- [ ] Avoids causation language (why, because, causes)

---

## Criterion 4: Methodology Plan (20%)

Evaluates alignment with CRISP-DM framework and appropriateness of planned analytical techniques.

| Score | Description |
|-------|-------------|
| **5 - Excellent** | Clear articulation of all CRISP-DM phases with specific activities planned for each. Appropriate EDA techniques identified for each research question. Timeline is realistic. Potential challenges acknowledged with mitigation strategies. |
| **4 - Good** | CRISP-DM phases are referenced with reasonable activities planned. EDA techniques are appropriate. Timeline is provided. Some awareness of potential challenges. |
| **3 - Acceptable** | CRISP-DM mentioned but not fully integrated into the plan. Some appropriate techniques identified. Timeline may be vague or overly optimistic. Limited discussion of challenges. |
| **2 - Below Expectations** | Methodology description is generic or missing CRISP-DM alignment. Techniques mentioned do not match the research questions. No timeline provided. |
| **1 - Not Acceptable** | No methodology plan provided, or the plan demonstrates fundamental misunderstanding of the analytical process. |

### CRISP-DM Phases to Address

| Phase | Expected Activities |
|-------|-------------------|
| Business Understanding | Problem definition, stakeholder identification |
| Data Understanding | Dataset exploration, data dictionary review |
| Data Preparation | Cleaning, handling missing values, transformations |
| Modeling | (For this course: EDA and visualization) |
| Evaluation | Validating insights, checking assumptions |
| Deployment | Presenting findings, recommendations |

---

## Criterion 5: Presentation Quality (15%)

Evaluates the clarity, organization, and professionalism of the oral presentation.

| Score | Description |
|-------|-------------|
| **5 - Excellent** | Presentation is well-organized, visually appealing, and stays within time limit. All required elements covered. Speakers are confident and engage the audience. Q&A responses are thoughtful. Team coordination is smooth. |
| **4 - Good** | Presentation covers all required elements within time limit. Slides are clear and readable. Speakers are prepared. Q&A responses are adequate. Minor coordination issues. |
| **3 - Acceptable** | Presentation covers most required elements. May slightly exceed time limit. Some slides are cluttered or hard to read. Speakers show adequate preparation. Q&A responses are brief or uncertain. |
| **2 - Below Expectations** | Presentation missing key elements or significantly over/under time. Slides are poorly designed. Speakers appear unprepared. Difficulty answering questions. |
| **1 - Not Acceptable** | Presentation not delivered, grossly incomplete, or demonstrates lack of preparation. Unable to answer basic questions about the project. |

### Presentation Checklist
- [ ] Title slide with team members
- [ ] Dataset overview (source, size, key variables)
- [ ] Problem statement and motivation
- [ ] Three research questions (clearly stated)
- [ ] Methodology plan (CRISP-DM reference)
- [ ] Within 8-minute time limit
- [ ] Professional visual design (readable fonts, consistent formatting)

---

## Scoring Sheet Template

**Team Name**: ________________________
**Date**: ________________________
**Evaluator**: ________________________

| Criterion | Score (1-5) | Weight | Weighted Score | Comments |
|-----------|-------------|--------|----------------|----------|
| Dataset Selection | | 20% | | |
| Problem Statement | | 20% | | |
| Research Questions | | 25% | | |
| Methodology Plan | | 20% | | |
| Presentation Quality | | 15% | | |
| **Total** | | **100%** | | |

### Calculation
Weighted Score = (Score x Weight) for each criterion
Final Score = Sum of Weighted Scores

### Grade Conversion

| Total Weighted Score | Grade |
|---------------------|-------|
| 4.5 - 5.0 | A (Excellent) |
| 4.0 - 4.4 | B (Good) |
| 3.0 - 3.9 | C (Acceptable) |
| 2.0 - 2.9 | D (Needs Improvement) |
| Below 2.0 | F (Unacceptable) |

---

## Feedback Template

### Strengths
1.
2.
3.

### Areas for Improvement
1.
2.
3.

### Required Revisions (if any)
- [ ]
- [ ]

### Deadline for Revisions
________________________

---

## Common Deductions

| Issue | Typical Impact |
|-------|---------------|
| Dataset not from datos.gov.co (unapproved) | -1 to -2 points on Dataset Selection |
| Fewer than 3 research questions | -1 point per missing question |
| No CRISP-DM reference | -1 to -2 points on Methodology |
| Presentation over 10 minutes | -1 point on Presentation Quality |
| Team member absent (unexcused) | -0.5 points on Presentation Quality |
| Plagiarism or copied content | Automatic 0 + academic integrity review |

---

## Appeals Process

Students may request a re-evaluation within 5 business days of receiving their score. Appeals must include:
1. Specific criterion being contested
2. Justification for why the score should be higher
3. Any supporting evidence not included in the original presentation

Re-evaluations will be conducted by the instructor and, if necessary, a second faculty member.
