{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 13: In-Class Exercise - Introduction to Machine Learning\n",
    "\n",
    "## Objective\n",
    "Prepare data for machine learning and build your first prediction model using the Water Consumption dataset.\n",
    "\n",
    "## Time: ~30 minutes\n",
    "\n",
    "## Dataset\n",
    "Water Consumption (HISTORICO_CONSUMO) - the same dataset we explored in Weeks 4 and 5.\n",
    "\n",
    "### What You Will Do:\n",
    "1. Define a prediction problem\n",
    "2. Prepare features (X) and target (y)\n",
    "3. Split data into training and testing sets\n",
    "4. Scale features using StandardScaler\n",
    "5. Build your first Decision Tree model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Run this cell to load the necessary libraries and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# Load the Water Consumption dataset\n",
    "url = \"https://www.datos.gov.co/api/views/wcpc-hgdr/rows.csv?accessType=DOWNLOAD\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Define the Prediction Problem (5 minutes)\n",
    "\n",
    "**Key Question:** What do we want to predict?\n",
    "\n",
    "For this exercise, we will create a **classification** problem:\n",
    "\n",
    "**Goal:** Predict whether a municipality has **HIGH** or **LOW** water consumption.\n",
    "\n",
    "We will define:\n",
    "- **HIGH consumption:** Above the median consumption\n",
    "- **LOW consumption:** Below or equal to the median consumption\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Task 1.1: Create the Target Variable\n",
    "\n",
    "First, let's look at the consumption distribution and create our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the consumption column\n",
    "consumption_col = 'CONSUMO_FACTURADO'\n",
    "\n",
    "print(f\"Consumption statistics:\")\n",
    "print(f\"  Mean: {df[consumption_col].mean():,.2f}\")\n",
    "print(f\"  Median: {df[consumption_col].median():,.2f}\")\n",
    "print(f\"  Min: {df[consumption_col].min():,.2f}\")\n",
    "print(f\"  Max: {df[consumption_col].max():,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target variable: HIGH vs LOW consumption\n",
    "median_consumption = df[consumption_col].median()\n",
    "\n",
    "# Create binary target: 1 = HIGH, 0 = LOW\n",
    "df['consumption_level'] = (df[consumption_col] > median_consumption).astype(int)\n",
    "\n",
    "# Check the distribution\n",
    "print(\"Target variable distribution:\")\n",
    "print(df['consumption_level'].value_counts())\n",
    "print(f\"\\n0 = LOW (below {median_consumption:,.0f} m3)\")\n",
    "print(f\"1 = HIGH (above {median_consumption:,.0f} m3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Pie chart\n",
    "labels = ['LOW', 'HIGH']\n",
    "sizes = df['consumption_level'].value_counts().sort_index()\n",
    "colors = ['steelblue', 'coral']\n",
    "axes[0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=14)\n",
    "\n",
    "# Bar chart\n",
    "axes[1].bar(labels, sizes, color=colors, edgecolor='black')\n",
    "axes[1].set_xlabel('Consumption Level', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Class Distribution', fontsize=14)\n",
    "\n",
    "for i, v in enumerate(sizes):\n",
    "    axes[1].text(i, v + 100, str(v), ha='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "**Question:** Why is having balanced classes (roughly 50/50) important for classification?\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Part 2: Prepare Features (X) and Target (y) (8 minutes)\n",
    "\n",
    "Now we need to select which features (input variables) we will use to predict consumption level.\n",
    "\n",
    "**Key concepts:**\n",
    "- **Features (X):** The input variables used to make predictions\n",
    "- **Target (y):** The variable we want to predict\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Task 2.1: Select Features\n",
    "\n",
    "Let's identify which columns can be used as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review available columns\n",
    "print(\"Available columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1}. {col} ({df[col].dtype})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numeric columns that could be features\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for our model\n",
    "# IMPORTANT: We cannot use CONSUMO_FACTURADO as a feature (that would be cheating!)\n",
    "# We also should not use VALOR_FACTURADO (highly correlated with consumption)\n",
    "\n",
    "# Let's use these features:\n",
    "feature_cols = ['NUMERO_SUSCRIPTORES', 'ANNO']  # Number of subscribers, Year\n",
    "\n",
    "# Check if we have a usage type column (USO) - we'll need to encode it\n",
    "if 'USO' in df.columns:\n",
    "    print(f\"\\nUsage types (USO):\")\n",
    "    print(df['USO'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variable (USO) if it exists\n",
    "if 'USO' in df.columns:\n",
    "    # Create a label encoder\n",
    "    le = LabelEncoder()\n",
    "    df['USO_encoded'] = le.fit_transform(df['USO'].fillna('UNKNOWN'))\n",
    "    \n",
    "    print(\"Encoding mapping:\")\n",
    "    for i, label in enumerate(le.classes_):\n",
    "        print(f\"  {label} -> {i}\")\n",
    "    \n",
    "    feature_cols.append('USO_encoded')\n",
    "\n",
    "print(f\"\\nFinal feature columns: {feature_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X (features) and y (target)\n",
    "# First, remove rows with missing values in our selected columns\n",
    "df_clean = df[feature_cols + ['consumption_level']].dropna()\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"After removing missing values: {len(df_clean)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean['consumption_level']\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeatures preview:\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Train/Test Split (5 minutes)\n",
    "\n",
    "**Why split the data?**\n",
    "\n",
    "Imagine studying for an exam by memorizing all the answers. You would score 100% on that exact test, but fail any new questions. This is called **overfitting**.\n",
    "\n",
    "To avoid this, we:\n",
    "1. **Train** the model on 80% of the data\n",
    "2. **Test** the model on 20% of data it has never seen\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Split the data\n",
    "# YOUR CODE HERE: Use train_test_split to split X and y\n",
    "# Use test_size=0.2 (20% for testing) and random_state=42 (for reproducibility)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=___,      # Fill in: What percentage for testing?\n",
    "    random_state=42     # Keep this for reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the class distribution is preserved in both sets\n",
    "print(\"Class distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "**Question:** Why do we use `random_state=42`?\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Part 4: Feature Scaling with StandardScaler (5 minutes)\n",
    "\n",
    "**Why scale features?**\n",
    "\n",
    "Different features have different scales:\n",
    "- Number of subscribers: 100 - 100,000\n",
    "- Year: 2016 - 2023\n",
    "\n",
    "Some algorithms (like Decision Trees) don't need scaling, but many others (like Neural Networks, SVM) require it.\n",
    "\n",
    "**StandardScaler** transforms data to have:\n",
    "- Mean = 0\n",
    "- Standard deviation = 1\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before scaling - check the current ranges\n",
    "print(\"Before scaling:\")\n",
    "print(X_train.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.1: Apply StandardScaler\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# IMPORTANT: fit_transform on training data, only transform on test data\n",
    "# YOUR CODE HERE:\n",
    "X_train_scaled = scaler.fit_transform(___)  # Fill in: Which data to fit and transform?\n",
    "X_test_scaled = scaler.transform(___)       # Fill in: Which data to only transform?\n",
    "\n",
    "# Convert back to DataFrame for easier viewing\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=feature_cols)\n",
    "\n",
    "print(\"After scaling (training data):\")\n",
    "print(X_train_scaled_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "**Important:** Notice that after scaling:\n",
    "- Mean is approximately 0\n",
    "- Standard deviation is approximately 1\n",
    "\n",
    "**Question:** Why do we `fit_transform` on training data but only `transform` on test data?\n",
    "\n",
    "*Your answer here*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "## Part 5: Build Your First Decision Tree (7 minutes)\n",
    "\n",
    "**What is a Decision Tree?**\n",
    "\n",
    "A Decision Tree is like playing \"20 Questions\":\n",
    "- Is the number of subscribers greater than 1000? Yes -> Go left, No -> Go right\n",
    "- Is it a residential usage type? Yes -> Predict LOW, No -> Check another question\n",
    "\n",
    "Each question (node) splits the data until we reach a prediction (leaf).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.1: Create and train the Decision Tree\n",
    "# Note: Decision Trees don't require scaled features, but we'll use the original for clarity\n",
    "\n",
    "# Create the model\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=3,        # Limit depth to make it interpretable\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model (fit on training data)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"\\nTree depth: {dt_model.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.2: Make predictions on the test set\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(f\"Predictions made: {len(y_pred)}\")\n",
    "print(f\"\\nSample predictions (first 10):\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Actual': y_test.head(10).values,\n",
    "    'Predicted': y_pred[:10],\n",
    "    'Correct': y_test.head(10).values == y_pred[:10]\n",
    "})\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.3: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"\\nThis means the model correctly predicted {accuracy*100:.1f}% of test cases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix - Shows where the model made mistakes\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['LOW', 'HIGH'], \n",
    "            yticklabels=['LOW', 'HIGH'],\n",
    "            ax=ax)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title('Confusion Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read the confusion matrix:\")\n",
    "print(f\"  - True Negatives (LOW correctly predicted as LOW): {cm[0,0]}\")\n",
    "print(f\"  - False Positives (LOW incorrectly predicted as HIGH): {cm[0,1]}\")\n",
    "print(f\"  - False Negatives (HIGH incorrectly predicted as LOW): {cm[1,0]}\")\n",
    "print(f\"  - True Positives (HIGH correctly predicted as HIGH): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model, \n",
    "    feature_names=feature_cols,\n",
    "    class_names=['LOW', 'HIGH'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Decision Tree Visualization', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nHow to read the tree:\")\n",
    "print(\"  - Each box shows a decision rule (e.g., 'subscribers <= 1000')\")\n",
    "print(\"  - 'gini' measures impurity (lower is better)\")\n",
    "print(\"  - 'samples' shows how many training examples reached this node\")\n",
    "print(\"  - 'value' shows [LOW count, HIGH count]\")\n",
    "print(\"  - 'class' is the predicted class for that node\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance - Which features matter most?\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.barh(importance['Feature'], importance['Importance'], color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Feature Importance', fontsize=14)\n",
    "ax.invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned the fundamental steps of machine learning:\n",
    "\n",
    "1. **Define the Problem**\n",
    "   - We converted consumption into a binary classification (HIGH/LOW)\n",
    "   - The target variable should be balanced for best results\n",
    "\n",
    "2. **Prepare Features (X) and Target (y)**\n",
    "   - Selected meaningful features that don't leak information\n",
    "   - Encoded categorical variables (USO)\n",
    "   - Removed missing values\n",
    "\n",
    "3. **Train/Test Split**\n",
    "   - 80% for training, 20% for testing\n",
    "   - random_state for reproducibility\n",
    "   - Prevents overfitting by testing on unseen data\n",
    "\n",
    "4. **Feature Scaling**\n",
    "   - StandardScaler normalizes features to mean=0, std=1\n",
    "   - fit_transform on train, transform on test\n",
    "\n",
    "5. **Build and Evaluate Model**\n",
    "   - Decision Tree is interpretable and easy to understand\n",
    "   - Confusion matrix shows where mistakes happen\n",
    "   - Feature importance shows what the model learned\n",
    "\n",
    "---\n",
    "\n",
    "*Next week, we will explore more models (Random Forest, Linear Regression) and learn how to compare them!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "1. **Why didn't we use CONSUMO_FACTURADO or VALOR_FACTURADO as features?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "2. **What does it mean if the model has 70% accuracy? Is that good or bad?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "3. **Looking at the feature importance, which feature was most important for predicting consumption level? Does this make sense?**\n",
    "\n",
    "   *Your answer here*\n",
    "\n",
    "---\n",
    "\n",
    "*End of Exercise*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
