{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 13 Workshop: Introduction to Machine Learning\n",
    "\n",
    "## ML Data Preparation for Water Consumption Prediction\n",
    "\n",
    "**Student Name:** (Your name here)\n",
    "\n",
    "**Date:** (Today's date)\n",
    "\n",
    "**Dataset:** Water Consumption (HISTORICO_CONSUMO) from datos.gov.co\n",
    "\n",
    "---\n",
    "\n",
    "### Workshop Objectives\n",
    "\n",
    "1. Define a clear ML problem statement\n",
    "2. Select and justify appropriate features\n",
    "3. Implement proper data preparation for ML\n",
    "4. Build and evaluate a baseline model\n",
    "5. Document all decisions with rationale\n",
    "\n",
    "### Duration: 2-3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this cell to load the necessary libraries and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:,.2f}'.format)\n",
    "\n",
    "# Load the Water Consumption dataset\n",
    "url = \"https://www.datos.gov.co/api/views/wcpc-hgdr/rows.csv?accessType=DOWNLOAD\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data types and missing values:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Problem Definition\n",
    "\n",
    "Before building any model, we must clearly define what we want to predict and why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Task 1.1: Explore Potential Target Variables\n",
    "\n",
    "What could we potentially predict from this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the main variables that could be targets\n",
    "print(\"Potential target variables:\")\n",
    "print(\"\\n1. CONSUMO_FACTURADO (Billed Consumption in m3):\")\n",
    "print(f\"   Range: {df['CONSUMO_FACTURADO'].min():,.0f} to {df['CONSUMO_FACTURADO'].max():,.0f}\")\n",
    "print(f\"   Mean: {df['CONSUMO_FACTURADO'].mean():,.2f}\")\n",
    "print(f\"   Median: {df['CONSUMO_FACTURADO'].median():,.2f}\")\n",
    "\n",
    "print(\"\\n2. VALOR_FACTURADO (Billed Amount in COP):\")\n",
    "print(f\"   Range: {df['VALOR_FACTURADO'].min():,.0f} to {df['VALOR_FACTURADO'].max():,.0f}\")\n",
    "print(f\"   Mean: {df['VALOR_FACTURADO'].mean():,.2f}\")\n",
    "\n",
    "print(\"\\n3. USO (Usage Type - Categorical):\")\n",
    "print(df['USO'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Task 1.2: Choose Classification vs Regression\n",
    "\n",
    "For this workshop, we will create a **classification** problem.\n",
    "\n",
    "**Goal:** Predict whether a municipality has HIGH or LOW water consumption.\n",
    "\n",
    "Define the target variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the target variable\n",
    "# Use the median as the threshold: above median = HIGH (1), below/equal = LOW (0)\n",
    "\n",
    "consumption_col = 'CONSUMO_FACTURADO'\n",
    "median_consumption = df[consumption_col].median()\n",
    "\n",
    "print(f\"Median consumption: {median_consumption:,.2f} m3\")\n",
    "\n",
    "# YOUR CODE HERE: Create the target variable\n",
    "df['consumption_level'] = ___  # Hint: (df[consumption_col] > median_consumption).astype(int)\n",
    "\n",
    "# Verify the target distribution\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "print(df['consumption_level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Pie chart\n",
    "labels = ['LOW', 'HIGH']\n",
    "sizes = df['consumption_level'].value_counts().sort_index()\n",
    "colors = ['steelblue', 'coral']\n",
    "\n",
    "# YOUR CODE HERE: Create a pie chart showing the class distribution\n",
    "axes[0].pie(___, labels=___, colors=___, autopct='%1.1f%%', startangle=90)\n",
    "axes[0].set_title('Target Variable Distribution', fontsize=14)\n",
    "\n",
    "# Bar chart\n",
    "# YOUR CODE HERE: Create a bar chart\n",
    "axes[1].bar(___, ___, color=colors, edgecolor='black')\n",
    "axes[1].set_xlabel('Consumption Level', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Class Distribution', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Task 1.3: Write Your Problem Statement\n",
    "\n",
    "Complete the formal problem statement below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "**Type of problem:** Classification (binary)\n",
    "\n",
    "**Target variable:** consumption_level (0 = LOW, 1 = HIGH)\n",
    "\n",
    "**Definition of classes:**\n",
    "- LOW: Consumption at or below _____ m3 (the median)\n",
    "- HIGH: Consumption above _____ m3\n",
    "\n",
    "**Goal:** Given a municipality's characteristics, predict whether their water consumption will be classified as HIGH or LOW.\n",
    "\n",
    "**Business context:** (Write 1-2 sentences about why this prediction would be useful)\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Task 1.4: Justify Your Choice\n",
    "\n",
    "**Why classification instead of regression?**\n",
    "\n",
    "*YOUR ANSWER HERE (Consider: interpretability, business use case, data characteristics)*\n",
    "\n",
    "**Why use the median as the threshold?**\n",
    "\n",
    "*YOUR ANSWER HERE (Consider: class balance, robustness to outliers)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Feature Selection\n",
    "\n",
    "Select which variables will be used as features (inputs) for prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Task 2.1: Identify All Potential Features\n",
    "\n",
    "List all columns and evaluate each as a potential feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review all columns\n",
    "print(\"All columns in the dataset:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    dtype = df[col].dtype\n",
    "    unique = df[col].nunique()\n",
    "    missing = df[col].isna().sum()\n",
    "    print(f\"{i+1}. {col:25} | Type: {str(dtype):10} | Unique: {unique:6} | Missing: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Task 2.2: Eliminate Data Leakage Features\n",
    "\n",
    "**Data leakage** occurs when information from outside the training dataset is used to create the model. This leads to overly optimistic results that don't generalize to new data.\n",
    "\n",
    "**Question:** Which columns should we DEFINITELY NOT use as features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Columns to EXCLUDE (data leakage risk):\n",
    "\n",
    "| Column | Reason for Exclusion |\n",
    "|--------|---------------------|\n",
    "| CONSUMO_FACTURADO | *YOUR ANSWER* (Hint: This IS what we're trying to predict) |\n",
    "| VALOR_FACTURADO | *YOUR ANSWER* (Hint: How is this related to consumption?) |\n",
    "| consumption_level | This is the target variable itself |\n",
    "| | *Add more if applicable* |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Task 2.3: Handle Categorical Variables\n",
    "\n",
    "Machine learning models require numeric inputs. We need to encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode the USO (usage type) column\n",
    "# We will use LabelEncoder for simplicity\n",
    "\n",
    "le_uso = LabelEncoder()\n",
    "\n",
    "# YOUR CODE HERE: Encode the USO column\n",
    "df['USO_encoded'] = le_uso.fit_transform(df['USO'].fillna('UNKNOWN'))\n",
    "\n",
    "print(\"Encoding mapping for USO:\")\n",
    "for i, label in enumerate(le_uso.classes_):\n",
    "    print(f\"  {label} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Encode DEPARTAMENTO if you want to use it\n",
    "# Note: This creates many categories - consider if it's worth including\n",
    "\n",
    "print(f\"\\nNumber of unique departments: {df['DEPARTAMENTO'].nunique()}\")\n",
    "\n",
    "# YOUR CODE HERE (optional): Encode DEPARTAMENTO\n",
    "# le_dept = LabelEncoder()\n",
    "# df['DEPARTAMENTO_encoded'] = le_dept.fit_transform(df['DEPARTAMENTO'].fillna('UNKNOWN'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Task 2.4: Document Feature Selection\n",
    "\n",
    "Complete the feature selection table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "### Feature Selection Table\n",
    "\n",
    "| Feature | Include? | Rationale |\n",
    "|---------|----------|----------|\n",
    "| NUMERO_SUSCRIPTORES | Yes/No | *YOUR RATIONALE* |\n",
    "| ANNO | Yes/No | *YOUR RATIONALE* |\n",
    "| USO_encoded | Yes/No | *YOUR RATIONALE* |\n",
    "| DEPARTAMENTO | Yes/No | *YOUR RATIONALE* |\n",
    "| MUNICIPIO | Yes/No | *YOUR RATIONALE* |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define your final feature list\n",
    "# Based on your analysis, select the features you will use\n",
    "\n",
    "feature_cols = [\n",
    "    # YOUR CODE HERE: List the features you selected\n",
    "    # Example: 'NUMERO_SUSCRIPTORES', 'ANNO', 'USO_encoded'\n",
    "]\n",
    "\n",
    "print(f\"Selected features ({len(feature_cols)}):\")\n",
    "for col in feature_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Data Preparation\n",
    "\n",
    "Prepare the data for machine learning with proper methodology.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Task 3.1: Handle Missing Values\n",
    "\n",
    "Check for and handle missing values in your selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in selected features\n",
    "print(\"Missing values in selected features:\")\n",
    "for col in feature_cols:\n",
    "    missing = df[col].isna().sum()\n",
    "    pct = missing / len(df) * 100\n",
    "    print(f\"  {col}: {missing} ({pct:.2f}%)\")\n",
    "\n",
    "# Also check target\n",
    "print(f\"\\n  consumption_level: {df['consumption_level'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove rows with missing values in selected columns\n",
    "# Create a clean dataset with only the columns we need\n",
    "\n",
    "cols_needed = feature_cols + ['consumption_level']\n",
    "df_clean = df[cols_needed].dropna()\n",
    "\n",
    "print(f\"Original dataset size: {len(df)}\")\n",
    "print(f\"After removing missing values: {len(df_clean)}\")\n",
    "print(f\"Rows removed: {len(df) - len(df_clean)} ({(len(df) - len(df_clean))/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "**Decision:** Did you drop rows or impute missing values? Why?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "## Task 3.2: Create X (Features) and y (Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create X and y\n",
    "\n",
    "X = df_clean[___]  # YOUR CODE: Which columns for features?\n",
    "y = df_clean[___]  # YOUR CODE: Which column for target?\n",
    "\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nFeatures preview:\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Task 3.3: Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Split the data into training and testing sets\n",
    "# Use 80% for training, 20% for testing\n",
    "# Set random_state=42 for reproducibility\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    ___,  # YOUR CODE: features\n",
    "    ___,  # YOUR CODE: target\n",
    "    test_size=___,     # YOUR CODE: what percentage for testing?\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify class distribution is preserved\n",
    "print(\"Class distribution:\")\n",
    "print(\"\\nOriginal:\")\n",
    "print(y.value_counts(normalize=True).round(3))\n",
    "print(\"\\nTraining set:\")\n",
    "print(y_train.value_counts(normalize=True).round(3))\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test.value_counts(normalize=True).round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "**Question:** Is the class distribution similar across all sets? Why is this important?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## Task 3.4: Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before scaling - check the current ranges\n",
    "print(\"Before scaling (training data):\")\n",
    "print(X_train.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply StandardScaler\n",
    "# IMPORTANT: fit_transform on training data, only transform on test data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "X_train_scaled = scaler.fit_transform(___)  # Which data to fit and transform?\n",
    "X_test_scaled = scaler.transform(___)       # Which data to only transform?\n",
    "\n",
    "# Convert to DataFrame for viewing\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_cols)\n",
    "\n",
    "print(\"After scaling (training data):\")\n",
    "print(X_train_scaled_df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "**Question:** Why do we fit the scaler ONLY on training data and not on the full dataset?\n",
    "\n",
    "*YOUR ANSWER HERE (Hint: Think about data leakage)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "## Task 3.5: Verify Data Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification\n",
    "print(\"=\" * 50)\n",
    "print(\"DATA PREPARATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. FEATURES:\")\n",
    "print(f\"   Selected: {feature_cols}\")\n",
    "print(f\"   Number of features: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\n2. DATASET SIZES:\")\n",
    "print(f\"   Training: {X_train_scaled.shape}\")\n",
    "print(f\"   Test: {X_test_scaled.shape}\")\n",
    "\n",
    "print(f\"\\n3. TARGET DISTRIBUTION:\")\n",
    "print(f\"   Train - LOW: {(y_train==0).sum()}, HIGH: {(y_train==1).sum()}\")\n",
    "print(f\"   Test  - LOW: {(y_test==0).sum()}, HIGH: {(y_test==1).sum()}\")\n",
    "\n",
    "print(f\"\\n4. SCALING:\")\n",
    "print(f\"   Mean of scaled training data: ~0\")\n",
    "print(f\"   Std of scaled training data: ~1\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Baseline Model\n",
    "\n",
    "Build a simple Decision Tree model as our baseline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "## Task 4.1: Train the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create and train the Decision Tree model\n",
    "# Note: Decision Trees don't need scaled features, but we'll use them for consistency\n",
    "\n",
    "# Create the model with limited depth for interpretability\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=3,        # Limit tree depth\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "\n",
    "# YOUR CODE HERE: Train the model\n",
    "dt_model.fit(___, ___)  # Which data to train on?\n",
    "\n",
    "print(\"Model trained successfully!\")\n",
    "print(f\"Tree depth: {dt_model.get_depth()}\")\n",
    "print(f\"Number of leaves: {dt_model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-47",
   "metadata": {},
   "source": [
    "## Task 4.2: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make predictions on the test set\n",
    "y_pred = dt_model.predict(___)  # Which data to predict on?\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['LOW', 'HIGH'],\n",
    "            yticklabels=['LOW', 'HIGH'],\n",
    "            ax=ax)\n",
    "ax.set_xlabel('Predicted', fontsize=12)\n",
    "ax.set_ylabel('Actual', fontsize=12)\n",
    "ax.set_title(f'Confusion Matrix (Accuracy: {accuracy:.2%})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"  True Negatives (LOW -> LOW): {cm[0,0]}\")\n",
    "print(f\"  False Positives (LOW -> HIGH): {cm[0,1]}\")\n",
    "print(f\"  False Negatives (HIGH -> LOW): {cm[1,0]}\")\n",
    "print(f\"  True Positives (HIGH -> HIGH): {cm[1,1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['LOW', 'HIGH']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-51",
   "metadata": {},
   "source": [
    "## Task 4.3: Interpret the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=feature_cols,\n",
    "    class_names=['LOW', 'HIGH'],\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    fontsize=10,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Decision Tree Visualization', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': feature_cols,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(importance.to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bars = ax.barh(importance['Feature'], importance['Importance'], color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title('Feature Importance', fontsize=14)\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, importance['Importance']):\n",
    "    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-54",
   "metadata": {},
   "source": [
    "## Task 4.4: Interpretation Questions\n",
    "\n",
    "Answer these questions based on your model results:\n",
    "\n",
    "**1. What is the overall accuracy? Is this good or bad?**\n",
    "\n",
    "*YOUR ANSWER HERE (Consider: What would random guessing achieve? Is our model better?)*\n",
    "\n",
    "**2. Which feature is most important for predicting consumption level?**\n",
    "\n",
    "*YOUR ANSWER HERE (Look at the feature importance chart)*\n",
    "\n",
    "**3. Looking at the confusion matrix, does the model make more false positives or false negatives?**\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "**4. What are 2-3 ways we could potentially improve this model?**\n",
    "\n",
    "*YOUR ANSWER HERE (Consider: more features, different model, more data, etc.)*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Documentation\n",
    "\n",
    "Summarize all your decisions and findings.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-56",
   "metadata": {},
   "source": [
    "## Summary: ML Data Preparation Decisions\n",
    "\n",
    "### Problem Definition\n",
    "\n",
    "| Aspect | Decision | Rationale |\n",
    "|--------|----------|----------|\n",
    "| Problem type | Classification | *YOUR RATIONALE* |\n",
    "| Target variable | consumption_level (HIGH/LOW) | *YOUR RATIONALE* |\n",
    "| Threshold | Median consumption | *YOUR RATIONALE* |\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "| Feature | Included | Type | Rationale |\n",
    "|---------|----------|------|----------|\n",
    "| NUMERO_SUSCRIPTORES | Yes/No | Numeric | *YOUR RATIONALE* |\n",
    "| ANNO | Yes/No | Numeric | *YOUR RATIONALE* |\n",
    "| USO_encoded | Yes/No | Encoded categorical | *YOUR RATIONALE* |\n",
    "| (Add others) | | | |\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "| Step | Decision | Details |\n",
    "|------|----------|--------|\n",
    "| Missing values | Drop/Impute | *How many rows affected?* |\n",
    "| Train/Test split | 80/20 | *random_state=42* |\n",
    "| Scaling | StandardScaler | *Mean=0, Std=1* |\n",
    "\n",
    "### Baseline Model Performance\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Accuracy | *YOUR VALUE* |\n",
    "| Precision (HIGH) | *YOUR VALUE* |\n",
    "| Recall (HIGH) | *YOUR VALUE* |\n",
    "| Most important feature | *YOUR VALUE* |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-57",
   "metadata": {},
   "source": [
    "## Next Steps for Week 14\n",
    "\n",
    "List 3-5 improvements or explorations for next week:\n",
    "\n",
    "1. *YOUR IDEA* (e.g., \"Try Random Forest model\")\n",
    "2. *YOUR IDEA* (e.g., \"Add more features like DEPARTAMENTO\")\n",
    "3. *YOUR IDEA* (e.g., \"Try regression instead of classification\")\n",
    "4. \n",
    "5. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-58",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "### 1. What was the most challenging part of preparing data for ML?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "### 2. Why is documentation of decisions important in ML projects?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "### 3. How would you explain your model to a non-technical stakeholder?\n",
    "\n",
    "*YOUR ANSWER HERE (Write 2-3 sentences)*\n",
    "\n",
    "### 4. What surprised you about the feature importance results?\n",
    "\n",
    "*YOUR ANSWER HERE*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "Before submitting, verify that you have:\n",
    "\n",
    "- [ ] All cells executed without errors (Kernel > Restart & Run All)\n",
    "- [ ] Problem statement clearly defined\n",
    "- [ ] Feature selection documented with rationale\n",
    "- [ ] Train/test split correctly implemented (80/20)\n",
    "- [ ] Feature scaling properly applied (fit on train only)\n",
    "- [ ] Baseline model trained and evaluated\n",
    "- [ ] Confusion matrix created and interpreted\n",
    "- [ ] Feature importance analyzed\n",
    "- [ ] All documentation sections completed\n",
    "- [ ] Reflection questions answered\n",
    "\n",
    "---\n",
    "\n",
    "*Week 13 Workshop - Data Analytics Course - Universidad Cooperativa de Colombia*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
