{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Week 14: In-Class Exercise - ML Models: Classification & Regression\n",
    "\n",
    "## Objective\n",
    "Implement and compare multiple machine learning models using the Water Consumption dataset.\n",
    "\n",
    "## Time: ~30 minutes\n",
    "\n",
    "## Dataset\n",
    "Water Consumption data from datos.gov.co - the same dataset we prepared in Week 13.\n",
    "\n",
    "### What You Will Do:\n",
    "1. Implement Linear Regression for consumption prediction\n",
    "2. Implement Decision Tree models\n",
    "3. Implement Random Forest models\n",
    "4. Compare model performance using appropriate metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Run this cell to load the necessary libraries and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# scikit-learn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, mean_absolute_error, r2_score,\n",
    "    accuracy_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Water Consumption dataset from datos.gov.co\n",
    "# Dataset: Consumo de Agua - contains water consumption records\n",
    "url = \"https://www.datos.gov.co/resource/k9gy-47jj.csv?$limit=10000\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data inspection\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nNumeric columns summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "Before building models, we need to:\n",
    "1. Identify numeric and categorical columns\n",
    "2. Handle missing values\n",
    "3. Select features and target variable\n",
    "4. Split data into train and test sets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential target and feature columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\")\n",
    "for col in numeric_cols:\n",
    "    print(f\"  - {col}: min={df[col].min():.2f}, max={df[col].max():.2f}, mean={df[col].mean():.2f}\")\n",
    "\n",
    "print(\"\\nCategorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  - {col}: {df[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target variable (consumption-related column)\n",
    "# Look for columns with names like: consumo, consumption, valor, cantidad\n",
    "consumption_candidates = [col for col in numeric_cols if any(x in col.lower() for x in ['consumo', 'consumption', 'valor', 'cantidad', 'total'])]\n",
    "print(f\"Potential target columns: {consumption_candidates}\")\n",
    "\n",
    "# Select the target column (update based on your dataset)\n",
    "if consumption_candidates:\n",
    "    target_col = consumption_candidates[0]\n",
    "else:\n",
    "    # Use the column with highest variance if no clear target\n",
    "    target_col = df[numeric_cols].var().idxmax()\n",
    "\n",
    "print(f\"\\nSelected target column: {target_col}\")\n",
    "print(f\"Target statistics:\")\n",
    "print(df[target_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "# Remove the target column and non-predictive columns from features\n",
    "\n",
    "# Feature columns: all numeric columns except target\n",
    "feature_cols = [col for col in numeric_cols if col != target_col]\n",
    "\n",
    "# Remove columns that are IDs or codes (usually not useful for prediction)\n",
    "id_patterns = ['id', 'codigo', 'code', 'key']\n",
    "feature_cols = [col for col in feature_cols if not any(p in col.lower() for p in id_patterns)]\n",
    "\n",
    "print(f\"Feature columns: {feature_cols}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Drop rows with missing values in selected columns\n",
    "df_clean = df[feature_cols + [target_col]].dropna()\n",
    "\n",
    "X = df_clean[feature_cols]\n",
    "y = df_clean[target_col]\n",
    "\n",
    "print(f\"Samples after cleaning: {len(X)}\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "# 80% training, 20% testing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Linear Regression (10 minutes)\n",
    "\n",
    "Linear Regression finds the best linear relationship between features and target.\n",
    "\n",
    "**Formula:** $y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Task 1.1: Train a Linear Regression Model\n",
    "\n",
    "Remember the sklearn pattern:\n",
    "1. Import (done above)\n",
    "2. Create the model\n",
    "3. Fit on training data\n",
    "4. Predict on test data\n",
    "5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.1: Create and train Linear Regression model\n",
    "\n",
    "# Step 1: Create the model\n",
    "# YOUR CODE HERE\n",
    "lr_model = ___\n",
    "\n",
    "# Step 2: Fit the model on training data\n",
    "# YOUR CODE HERE: lr_model.fit(X_train, y_train)\n",
    "___\n",
    "\n",
    "print(\"Linear Regression model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make predictions\n",
    "# YOUR CODE HERE\n",
    "y_pred_lr = ___\n",
    "\n",
    "print(f\"First 5 predictions: {y_pred_lr[:5]}\")\n",
    "print(f\"First 5 actual values: {y_test.values[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the model\n",
    "# For regression, we use: RMSE, MAE, and R-squared\n",
    "\n",
    "# Calculate metrics\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"=== LINEAR REGRESSION RESULTS ===\")\n",
    "print(f\"RMSE (Root Mean Squared Error): {rmse_lr:.4f}\")\n",
    "print(f\"MAE (Mean Absolute Error): {mae_lr:.4f}\")\n",
    "print(f\"R-squared: {r2_lr:.4f}\")\n",
    "print(f\"\\nInterpretation: The model explains {r2_lr*100:.1f}% of the variance in the target.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Actual vs Predicted\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.scatter(y_test, y_pred_lr, alpha=0.5, color='steelblue')\n",
    "ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "\n",
    "ax.set_xlabel('Actual Values', fontsize=12)\n",
    "ax.set_ylabel('Predicted Values', fontsize=12)\n",
    "ax.set_title(f'Linear Regression: Actual vs Predicted\\nR-squared = {r2_lr:.4f}', fontsize=14)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Decision Tree (10 minutes)\n",
    "\n",
    "Decision Trees make predictions by learning simple decision rules from the data.\n",
    "\n",
    "**Key parameters:**\n",
    "- `max_depth`: Maximum depth of the tree (prevents overfitting)\n",
    "- `min_samples_split`: Minimum samples required to split a node\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Task 2.1: Train a Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1: Create and train Decision Tree model\n",
    "\n",
    "# Step 1: Create the model with max_depth=5 to prevent overfitting\n",
    "# YOUR CODE HERE: dt_model = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "dt_model = ___\n",
    "\n",
    "# Step 2: Fit the model\n",
    "# YOUR CODE HERE\n",
    "___\n",
    "\n",
    "print(\"Decision Tree model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make predictions\n",
    "# YOUR CODE HERE\n",
    "y_pred_dt = ___\n",
    "\n",
    "# Step 4: Evaluate\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "r2_dt = r2_score(y_test, y_pred_dt)\n",
    "\n",
    "print(\"=== DECISION TREE RESULTS ===\")\n",
    "print(f\"RMSE: {rmse_dt:.4f}\")\n",
    "print(f\"MAE: {mae_dt:.4f}\")\n",
    "print(f\"R-squared: {r2_dt:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting: Compare train vs test performance\n",
    "y_pred_train_dt = dt_model.predict(X_train)\n",
    "r2_train_dt = r2_score(y_train, y_pred_train_dt)\n",
    "\n",
    "print(f\"Training R-squared: {r2_train_dt:.4f}\")\n",
    "print(f\"Test R-squared: {r2_dt:.4f}\")\n",
    "print(f\"Gap: {r2_train_dt - r2_dt:.4f}\")\n",
    "\n",
    "if r2_train_dt - r2_dt > 0.1:\n",
    "    print(\"\\nWarning: Possible overfitting! Consider reducing max_depth.\")\n",
    "else:\n",
    "    print(\"\\nGood: No significant overfitting detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Random Forest (10 minutes)\n",
    "\n",
    "Random Forest combines multiple Decision Trees to improve predictions and reduce overfitting.\n",
    "\n",
    "**Key parameters:**\n",
    "- `n_estimators`: Number of trees in the forest\n",
    "- `max_depth`: Maximum depth of each tree\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "### Task 3.1: Train a Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Create and train Random Forest model\n",
    "\n",
    "# Step 1: Create the model\n",
    "# YOUR CODE HERE: rf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_model = ___\n",
    "\n",
    "# Step 2: Fit the model\n",
    "# YOUR CODE HERE\n",
    "___\n",
    "\n",
    "print(\"Random Forest model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Make predictions\n",
    "# YOUR CODE HERE\n",
    "y_pred_rf = ___\n",
    "\n",
    "# Step 4: Evaluate\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"=== RANDOM FOREST RESULTS ===\")\n",
    "print(f\"RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"MAE: {mae_rf:.4f}\")\n",
    "print(f\"R-squared: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance from Random Forest\n",
    "# This tells us which features are most important for predictions\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Feature Importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.barh(feature_importance['feature'], feature_importance['importance'], color='steelblue')\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "ax.set_title('Random Forest: Feature Importance', fontsize=14)\n",
    "ax.invert_yaxis()  # Highest importance at top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Model Comparison\n",
    "\n",
    "Compare all three models side by side.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Linear Regression', 'Decision Tree', 'Random Forest'],\n",
    "    'RMSE': [rmse_lr, rmse_dt, rmse_rf],\n",
    "    'MAE': [mae_lr, mae_dt, mae_rf],\n",
    "    'R-squared': [r2_lr, r2_dt, r2_rf]\n",
    "})\n",
    "\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Identify best model\n",
    "best_model = comparison.loc[comparison['R-squared'].idxmax(), 'Model']\n",
    "print(f\"\\nBest Model (highest R-squared): {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5))\n",
    "\n",
    "models = ['Linear Regression', 'Decision Tree', 'Random Forest']\n",
    "colors = ['steelblue', 'coral', 'seagreen']\n",
    "\n",
    "# RMSE comparison\n",
    "axes[0].bar(models, [rmse_lr, rmse_dt, rmse_rf], color=colors)\n",
    "axes[0].set_ylabel('RMSE (lower is better)', fontsize=11)\n",
    "axes[0].set_title('RMSE Comparison', fontsize=13)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(models, [mae_lr, mae_dt, mae_rf], color=colors)\n",
    "axes[1].set_ylabel('MAE (lower is better)', fontsize=11)\n",
    "axes[1].set_title('MAE Comparison', fontsize=13)\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# R-squared comparison\n",
    "axes[2].bar(models, [r2_lr, r2_dt, r2_rf], color=colors)\n",
    "axes[2].set_ylabel('R-squared (higher is better)', fontsize=11)\n",
    "axes[2].set_title('R-squared Comparison', fontsize=13)\n",
    "axes[2].tick_params(axis='x', rotation=15)\n",
    "axes[2].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions comparison plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "predictions = [y_pred_lr, y_pred_dt, y_pred_rf]\n",
    "r2_scores = [r2_lr, r2_dt, r2_rf]\n",
    "\n",
    "for ax, pred, model, r2, color in zip(axes, predictions, models, r2_scores, colors):\n",
    "    ax.scatter(y_test, pred, alpha=0.5, color=color, s=30)\n",
    "    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
    "    ax.set_xlabel('Actual', fontsize=11)\n",
    "    ax.set_ylabel('Predicted', fontsize=11)\n",
    "    ax.set_title(f'{model}\\nR-squared = {r2:.4f}', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, you learned:\n",
    "\n",
    "1. **Linear Regression**\n",
    "   - Finds linear relationships between features and target\n",
    "   - Simple and interpretable\n",
    "   - Works best when relationships are truly linear\n",
    "\n",
    "2. **Decision Tree**\n",
    "   - Learns decision rules from data\n",
    "   - Can capture non-linear patterns\n",
    "   - Prone to overfitting without depth limits\n",
    "\n",
    "3. **Random Forest**\n",
    "   - Ensemble of many decision trees\n",
    "   - More robust and accurate than single tree\n",
    "   - Provides feature importance\n",
    "\n",
    "4. **Regression Metrics**\n",
    "   - RMSE: Average error magnitude (same units as target)\n",
    "   - MAE: Average absolute error (easier to interpret)\n",
    "   - R-squared: Proportion of variance explained (0-1)\n",
    "\n",
    "---\n",
    "\n",
    "*End of Exercise*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
