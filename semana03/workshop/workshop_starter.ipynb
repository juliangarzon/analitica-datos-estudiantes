{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 3 Workshop: Data Cleaning Practice\n",
        "\n",
        "**Duration:** ~2 hours (independent work)  \n",
        "**Dataset:** Education Statistics from the Colombian Ministry of Education (datos.gov.co)  \n",
        "**Rows:** 482 (dirty) | **Columns:** 37  \n",
        "\n",
        "---\n",
        "\n",
        "### What is this?\n",
        "\n",
        "This is your **independent practice** notebook. Unlike the in-class exercise where code was provided for you to run,\n",
        "here **you write all the code yourself**. Each section tells you what to do, gives you hints about which pandas\n",
        "methods to use, and describes what your output should look like. But the actual code is yours to write.\n",
        "\n",
        "### How to work through this notebook\n",
        "\n",
        "1. **Read** the markdown cell explaining the task\n",
        "2. **Write** your code in the code cell (follow the comments for structure)\n",
        "3. **Run** your code and compare with the expected output described\n",
        "4. **Document** your decisions in the reflection cells (these matter for grading)\n",
        "\n",
        "### Grading\n",
        "\n",
        "You are graded on **two things equally**:\n",
        "- Correct, working code that cleans the dataset\n",
        "- Thoughtful documentation of your decisions (the markdown cells asking for your reasoning)\n",
        "\n",
        "### The 5 data quality issues you must fix\n",
        "\n",
        "| # | Issue | Columns affected |\n",
        "|---|-------|------------------|\n",
        "| 1 | Missing values (NaN) | Multiple rate columns, `departamento`, `tamano_promedio_grupo`, `sedes_conectadas_a_internet` |\n",
        "| 2 | Wrong data types | `ano` (float instead of int), `poblacion_5_16` (text instead of number) |\n",
        "| 3 | Duplicate rows | ~20 exact duplicates |\n",
        "| 4 | Text inconsistencies | `departamento` has ~80+ unique values instead of ~34 |\n",
        "| 5 | Invalid values | Negative percentages and percentages over 100 |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Run this cell as-is. It imports the libraries, loads the dataset, and stores a copy of the original\n",
        "so you can compare before and after at the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('../data/educacion_estadisticas.csv')\n",
        "\n",
        "# Store original for comparison at the end\n",
        "df_original = df.copy()\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1: Initial Inspection\n",
        "\n",
        "Before you clean anything, you need to **diagnose** the problems. This is the inspection ritual\n",
        "you should run at the start of every data project:\n",
        "\n",
        "1. `df.shape` -- how big is the dataset?\n",
        "2. `df.head()` -- what does the actual data look like?\n",
        "3. `df.dtypes` -- are columns the right type?\n",
        "4. `df.isnull().sum()` -- where are the gaps?\n",
        "5. `df.describe()` -- are the numbers reasonable?\n",
        "\n",
        "Run all five in the cell below. The first line is done for you. Complete the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Shape\n",
        "print(f\"Rows: {df.shape[0]}, Columns: {df.shape[1]}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# 2. First rows -- display the first 5 rows\n",
        "# YOUR CODE\n",
        "\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Data types -- display all column types\n",
        "# Look for: ano should be int but is not. poblacion_5_16 should be numeric but is not.\n",
        "# YOUR CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Missing values -- count NaN per column, sorted from worst to least\n",
        "# Hint: df.isnull().sum().sort_values(ascending=False)\n",
        "# Only show columns that have at least 1 missing value\n",
        "# YOUR CODE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Statistical summary -- run describe() and look at min/max values\n",
        "# Are there any negative percentages? Any values over 100 that shouldn't be?\n",
        "# YOUR CODE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation: Initial Inspection\n",
        "\n",
        "Based on the 5 commands you just ran, list the data quality issues you found.\n",
        "For each one, note which column(s) are affected, what the problem is, and which command revealed it.\n",
        "\n",
        "| # | Issue | Column(s) | How you spotted it |\n",
        "|---|-------|-----------|--------------------|\n",
        "| 1 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "| 2 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "| 3 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "| 4 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "| 5 | *YOUR ANSWER* | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Missing Values\n",
        "\n",
        "Missing values (NaN) prevent calculations and distort results. But not all missing values\n",
        "should be handled the same way. The strategy depends on **what the column represents**\n",
        "and **how much data is missing**.\n",
        "\n",
        "Use this decision framework:\n",
        "\n",
        "| Missing % | Recommended action | Reasoning |\n",
        "|-----------|-------------------|-----------|\n",
        "| > 50% | Consider dropping the column, or fill with 0 if \"not reported\" makes sense | More gaps than data |\n",
        "| < 5% | Drop the rows | Losing very few rows |\n",
        "| 5-50% | Fill with an appropriate value (median for rates, 0 for counts) | Too many rows to lose |\n",
        "\n",
        "You need to handle 3 groups of missing values:\n",
        "\n",
        "1. **Count columns** (`sedes_conectadas_a_internet`, `tamano_promedio_grupo`): ~50% missing. These columns only have data through 2017. Fill with 0 (\"not reported\").\n",
        "2. **`departamento`**: ~3% missing. This is the key identifier. Drop those rows (you cannot guess which department a row belongs to).\n",
        "3. **Rate columns** (dropout, coverage, approval, etc.): ~8-11% missing. Fill with the median (the middle value, resistant to outliers).\n",
        "\n",
        "**Why median instead of mean for rates?** The mean is pulled by extreme outliers. The median is the\n",
        "middle value and better represents \"typical.\" For education rates, median is the safer assumption.\n",
        "\n",
        "**Why NOT fill rates with 0?** A 0% dropout rate means \"nobody dropped out\" (a strong claim).\n",
        "That is very different from \"we don't know.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.1: Calculate missing percentages\n",
        "\n",
        "Calculate the percentage of missing values per column. Show only columns with at least 1 missing value,\n",
        "sorted from highest to lowest.\n",
        "\n",
        "**Hint:** `(df.isnull().sum() / len(df) * 100).round(2)`\n",
        "\n",
        "**Expected output:** You should see `sedes_conectadas_a_internet` and `tamano_promedio_grupo` around 50%,\n",
        "`departamento` around 2-3%, and several rate columns around 8-11%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate percentage of missing values per column\n",
        "missing_pct = # YOUR CODE: (df.isnull().sum() / len(df) * 100).round(2)\n",
        "\n",
        "# Show only columns with missing values, sorted descending\n",
        "# YOUR CODE: filter to only > 0, sort descending, print\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.2: Fill count-based columns with 0\n",
        "\n",
        "The columns `sedes_conectadas_a_internet` (% schools with internet) and `tamano_promedio_grupo`\n",
        "(average class size) are only available through 2017. After that, they were not reported.\n",
        "Fill them with 0 to indicate \"no data available.\"\n",
        "\n",
        "**Hint:** `df['column'] = df['column'].fillna(0)`\n",
        "\n",
        "**Expected output:** Both columns should go from ~240 NaN to 0 NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print NaN count before\n",
        "print(\"Before:\")\n",
        "print(f\"  sedes_conectadas_a_internet NaN: {df['sedes_conectadas_a_internet'].isnull().sum()}\")\n",
        "print(f\"  tamano_promedio_grupo NaN:       {df['tamano_promedio_grupo'].isnull().sum()}\")\n",
        "\n",
        "# YOUR CODE: fill both columns with 0\n",
        "\n",
        "\n",
        "# Print NaN count after\n",
        "print(\"\\nAfter:\")\n",
        "print(f\"  sedes_conectadas_a_internet NaN: {df['sedes_conectadas_a_internet'].isnull().sum()}\")\n",
        "print(f\"  tamano_promedio_grupo NaN:       {df['tamano_promedio_grupo'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.3: Drop rows with missing `departamento`\n",
        "\n",
        "The `departamento` column is the key identifier. A row without a department is like a letter\n",
        "without an address: we cannot use it. Drop those rows.\n",
        "\n",
        "**Hint:** `df = df.dropna(subset=['departamento'])`\n",
        "\n",
        "**Expected output:** You should remove approximately 10-15 rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows_before = len(df)\n",
        "\n",
        "# YOUR CODE: drop rows where departamento is NaN\n",
        "\n",
        "\n",
        "rows_after = len(df)\n",
        "print(f\"Rows before: {rows_before}\")\n",
        "print(f\"Rows after:  {rows_after}\")\n",
        "print(f\"Removed {rows_before - rows_after} rows with missing departamento\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.4: Fill rate columns with the median\n",
        "\n",
        "Rate columns represent percentages (dropout rate, coverage, approval, etc.). For these,\n",
        "filling with the median is the safest strategy: \"if we don't know, assume a typical value.\"\n",
        "\n",
        "The list of rate columns is provided below. Loop through each one, check if it has\n",
        "missing values, and fill them with that column's median.\n",
        "\n",
        "**Hint:** For each column, use `df[col].fillna(df[col].median())` to fill NaN with the median.\n",
        "\n",
        "**Expected output:** Each column should print how many NaN were filled and what median was used.\n",
        "After the loop, all rate columns should have 0 NaN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rate_columns = [\n",
        "    'tasa_matriculacion_5_16',\n",
        "    'cobertura_neta', 'cobertura_neta_transicion', 'cobertura_neta_primaria',\n",
        "    'cobertura_neta_secundaria', 'cobertura_neta_media',\n",
        "    'cobertura_bruta', 'cobertura_bruta_transicion', 'cobertura_bruta_primaria',\n",
        "    'cobertura_bruta_secundaria', 'cobertura_bruta_media',\n",
        "    'desercion', 'desercion_transicion', 'desercion_primaria',\n",
        "    'desercion_secundaria', 'desercion_media',\n",
        "    'aprobacion', 'aprobacion_transicion', 'aprobacion_primaria',\n",
        "    'aprobacion_secundaria', 'aprobacion_media',\n",
        "    'reprobacion', 'reprobacion_transicion', 'reprobacion_primaria',\n",
        "    'reprobacion_secundaria', 'reprobacion_media',\n",
        "    'repitencia', 'repitencia_transicion', 'repitencia_primaria',\n",
        "    'repitencia_secundaria', 'repitencia_media',\n",
        "]\n",
        "\n",
        "# YOUR CODE: loop through rate_columns\n",
        "#   For each column:\n",
        "#     1. Count how many NaN it has\n",
        "#     2. If it has any NaN, calculate the median\n",
        "#     3. Fill NaN with the median\n",
        "#     4. Print: \"{column}: filled {n} NaN with median {value:.2f}\"\n",
        "\n",
        "\n",
        "\n",
        "# Verify: total NaN remaining in rate columns\n",
        "print(f\"\\nTotal NaN remaining in rate columns: {df[rate_columns].isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.5: Verify missing values\n",
        "\n",
        "Check how many total NaN remain in the entire dataset.\n",
        "\n",
        "**Expected output:** The only column with remaining NaN should be `poblacion_5_16` (we fix\n",
        "that in Part 3, because it has both missing values AND type problems)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: show remaining NaN per column (only columns with > 0)\n",
        "remaining_nan = df.isnull().sum()\n",
        "remaining_nan = remaining_nan[remaining_nan > 0]\n",
        "\n",
        "if len(remaining_nan) == 0:\n",
        "    print(\"No missing values remain!\")\n",
        "else:\n",
        "    print(f\"Columns still with NaN ({len(remaining_nan)}):\")\n",
        "    print(remaining_nan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation: Missing Values\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "**1. Which fill strategy did you use for each type of column?**\n",
        "\n",
        "| Column type | Strategy | Why |\n",
        "|-------------|----------|-----|\n",
        "| Count columns (sedes, tamano) | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "| departamento | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "| Rate columns | *YOUR ANSWER* | *YOUR ANSWER* |\n",
        "\n",
        "**2. Why did you choose median over mean for rate columns?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**3. How many missing values did `departamento` have? Why is dropping rows acceptable here but not for rate columns?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Data Type Issues\n",
        "\n",
        "Even if a column contains numbers, pandas might store it as text (`object`) if even one value\n",
        "has non-numeric characters. And a column of whole numbers will be stored as `float64` (with\n",
        "decimal points) if it contains any NaN values, because NaN is technically a float.\n",
        "\n",
        "You need to fix two columns:\n",
        "\n",
        "1. **`ano`** (year): Currently `float64` (shows as 2023.0). Should be `int64` (2023).\n",
        "2. **`poblacion_5_16`** (population aged 5-16): Currently `object` (text) because some values have commas like \"394,574\" and some say \"sin dato\". Should be `int64`.\n",
        "\n",
        "**Key methods:**\n",
        "- `pd.to_numeric(series, errors='coerce')` -- converts to number, turns unparseable values into NaN\n",
        "- `.str.replace(',', '')` -- removes commas from strings\n",
        "- `.fillna(0).astype(int)` -- fills NaN then converts to integer\n",
        "\n",
        "**Why `errors='coerce'`?** Without it, `pd.to_numeric()` crashes when it hits a non-numeric\n",
        "value like \"sin dato\". With `errors='coerce'`, it quietly turns those into NaN instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.1: Fix `ano` (year)\n",
        "\n",
        "Convert `ano` from float to integer. The process is:\n",
        "1. Use `pd.to_numeric(errors='coerce')` to handle any non-numeric values\n",
        "2. Fill remaining NaN with 0\n",
        "3. Convert to int with `.astype(int)`\n",
        "\n",
        "**Expected output:** `ano` dtype changes from `float64` to `int64`. Sample values like 2023.0 become 2023."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Before: ano dtype = {df['ano'].dtype}\")\n",
        "print(f\"Sample: {df['ano'].head(5).tolist()}\")\n",
        "\n",
        "# YOUR CODE: convert ano to int\n",
        "#   1. pd.to_numeric() with errors='coerce'\n",
        "#   2. fillna(0)\n",
        "#   3. astype(int)\n",
        "\n",
        "\n",
        "print(f\"\\nAfter: ano dtype = {df['ano'].dtype}\")\n",
        "print(f\"Sample: {df['ano'].head(5).tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.2: Fix `poblacion_5_16` (population)\n",
        "\n",
        "This column is trickier. It is stored as text (`object`) because:\n",
        "- Some values have commas: \"394,574\"\n",
        "- Some values say \"sin dato\" (\"no data\" in Spanish)\n",
        "\n",
        "The process is:\n",
        "1. Convert to string with `.astype(str)` (ensures all values are strings for `.str.replace()`)\n",
        "2. Remove commas with `.str.replace(',', '')`\n",
        "3. Convert to numeric with `pd.to_numeric(errors='coerce')` (\"sin dato\" becomes NaN)\n",
        "4. Fill NaN with 0 and convert to int\n",
        "\n",
        "**Expected output:** `poblacion_5_16` dtype changes from `object` to `int64`. Values like \"394,574\" become 394574."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Before: poblacion_5_16 dtype = {df['poblacion_5_16'].dtype}\")\n",
        "print(f\"Sample: {df['poblacion_5_16'].head(5).tolist()}\")\n",
        "\n",
        "# YOUR CODE: convert poblacion_5_16 to int\n",
        "#   1. .astype(str) to make sure everything is a string\n",
        "#   2. .str.replace(',', '') to remove commas\n",
        "#   3. pd.to_numeric(errors='coerce') to convert (\"sin dato\" becomes NaN)\n",
        "#   4. .fillna(0).astype(int)\n",
        "\n",
        "\n",
        "print(f\"\\nAfter: poblacion_5_16 dtype = {df['poblacion_5_16'].dtype}\")\n",
        "print(f\"Sample: {df['poblacion_5_16'].head(5).tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.3: Verify type fixes\n",
        "\n",
        "Print the dtypes of `ano` and `poblacion_5_16` to confirm they are both `int64`.\n",
        "\n",
        "**Expected output:** Both should be `int64`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: print dtype for both columns and verify they are int64\n",
        "print(f\"ano:             {df['ano'].dtype}\")\n",
        "print(f\"poblacion_5_16:  {df['poblacion_5_16'].dtype}\")\n",
        "\n",
        "# Also check: any remaining NaN in these columns?\n",
        "print(f\"\\nano NaN:             {df['ano'].isnull().sum()}\")\n",
        "print(f\"poblacion_5_16 NaN:  {df['poblacion_5_16'].isnull().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation: Data Types\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "**1. What would happen if you tried `df['ano'].astype(int)` directly on a column that still has NaN values?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**2. Why do we need `errors='coerce'` in `pd.to_numeric()`? What would happen without it when the column contains \"sin dato\"?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**3. Why do we convert to string first (`.astype(str)`) before using `.str.replace()`?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Duplicate Rows\n",
        "\n",
        "Duplicate rows inflate counts and distort averages. If a department appears twice for the\n",
        "same year with identical data, every calculation using that data is biased. The dataset\n",
        "should have ~462 rows (one per department per year), but it has more because of duplicates.\n",
        "\n",
        "**Key methods:**\n",
        "- `df.duplicated().sum()` -- count how many duplicate rows exist\n",
        "- `df[df.duplicated(keep=False)]` -- show ALL copies (both \"original\" and \"duplicate\")\n",
        "- `df.drop_duplicates()` -- keep the first occurrence, remove the rest\n",
        "\n",
        "**Expected result:** You should find approximately 20 duplicate rows. After removal, expect ~462 rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.1: Count duplicates\n",
        "\n",
        "How many exact duplicate rows are in the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: count duplicate rows\n",
        "n_dupes = # YOUR CODE\n",
        "print(f\"Duplicate rows: {n_dupes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.2: Examine the duplicates\n",
        "\n",
        "Look at the actual duplicate rows to understand what they are. Use `keep=False` to see\n",
        "both the original and the copy side by side. Sort by `departamento` and `ano` to group them.\n",
        "\n",
        "Show only the key columns: `ano`, `departamento`, `poblacion_5_16`, `desercion`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: show all duplicate rows, sorted by departamento and ano\n",
        "# Display columns: ano, departamento, poblacion_5_16, desercion\n",
        "# Hint: df[df.duplicated(keep=False)].sort_values([...])[[...]]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.3: Remove duplicates\n",
        "\n",
        "Remove the duplicate rows, keeping the first occurrence of each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rows_before = len(df)\n",
        "\n",
        "# YOUR CODE: remove duplicates\n",
        "\n",
        "\n",
        "rows_after = len(df)\n",
        "print(f\"Rows before: {rows_before}\")\n",
        "print(f\"Rows after:  {rows_after}\")\n",
        "print(f\"Removed {rows_before - rows_after} duplicate rows\")\n",
        "print(f\"Duplicates remaining: {df.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation: Duplicates\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "**1. How many duplicate rows did you find? Are they exact copies (all columns identical)?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**2. When might a duplicate row be valid and NOT an error? Give one real-world example.**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Text Inconsistencies\n",
        "\n",
        "Colombia has about 34 departments (32 states + Bogota D.C. + a national aggregate). But our\n",
        "`departamento` column has far more unique values because the same department appears in\n",
        "different forms:\n",
        "\n",
        "- \"Antioquia\", \"ANTIOQUIA\", \"  Antioquia  \", \"antioquia\"\n",
        "- Leading/trailing spaces\n",
        "- Mixed upper/lower case\n",
        "\n",
        "To pandas, each of these is a completely different string. This breaks grouping: if you\n",
        "try `df.groupby('departamento')`, you get 80+ groups instead of 34.\n",
        "\n",
        "**Key methods:**\n",
        "- `.str.upper()` -- convert to uppercase\n",
        "- `.str.strip()` -- remove leading/trailing whitespace\n",
        "- `.nunique()` -- count unique values\n",
        "\n",
        "**IMPORTANT:** After standardizing text, previously different rows (\"Antioquia\" and \"ANTIOQUIA\"\n",
        "for the same year) become identical. You must check for NEW duplicates after this step.\n",
        "\n",
        "**Expected result:** Unique departments should drop from ~80+ to ~34."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.1: Check current state\n",
        "\n",
        "How many unique department values exist before standardization? Print the count and the\n",
        "sorted list of all unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Unique departments before: {df['departamento'].nunique()}\")\n",
        "print(f\"(Expected: ~34)\")\n",
        "print()\n",
        "\n",
        "# YOUR CODE: print sorted list of all unique department values\n",
        "# Hint: sorted(df['departamento'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.2: Standardize text\n",
        "\n",
        "Apply `.str.upper().str.strip()` to the `departamento` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: standardize departamento to uppercase and stripped\n",
        "\n",
        "\n",
        "print(f\"Unique departments after: {df['departamento'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.3: Check for NEW duplicates\n",
        "\n",
        "Cleaning text can reveal new problems. Rows that looked different before (\"Antioquia\" vs\n",
        "\"ANTIOQUIA\" for the same year) are now identical. Check for and remove any new duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: check for new duplicates\n",
        "new_dupes = # YOUR CODE\n",
        "print(f\"New duplicates after text fix: {new_dupes}\")\n",
        "\n",
        "# YOUR CODE: if there are new duplicates, remove them\n",
        "\n",
        "\n",
        "print(f\"Final row count: {len(df)}\")\n",
        "print(f\"Duplicates remaining: {df.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5.4: Verify final department list\n",
        "\n",
        "Print the sorted list of unique departments. It should look like a clean list of\n",
        "Colombian departments, all uppercase, no duplicates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: print sorted unique departments after cleaning\n",
        "print(f\"Unique departments: {df['departamento'].nunique()}\")\n",
        "print()\n",
        "\n",
        "# YOUR CODE: print the sorted list\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation: Text Inconsistencies\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "**1. How many unique department values did you start with, and how many after cleaning?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**2. Did text standardization create any new duplicates? How many?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**3. What other text cleaning might still be needed? (Think about accents: Narino vs. Nari\\u00f1o)**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Invalid Values\n",
        "\n",
        "This is the trickiest issue because the values are not missing, not the wrong type, and\n",
        "not duplicates. They **exist** and **look like numbers**, but they are **logically impossible**.\n",
        "\n",
        "Percentage columns like dropout rate, coverage, and approval must be between 0 and 100:\n",
        "- A dropout rate of **-5%** is impossible (you cannot have negative dropouts)\n",
        "- A net coverage of **150%** is impossible (net coverage caps at 100%)\n",
        "\n",
        "These errors slip past all automated checks. Only **domain knowledge** (knowing what\n",
        "valid education statistics look like) can catch them.\n",
        "\n",
        "**Strategy:** Replace invalid values with NaN, then fill with the column median (same\n",
        "approach we used for missing values).\n",
        "\n",
        "**Key methods:**\n",
        "- `df[cols].describe().loc[['min', 'max']]` -- check ranges quickly\n",
        "- `df[col] < 0` -- boolean mask for negative values\n",
        "- `df[col] > 100` -- boolean mask for values over 100\n",
        "- `df.loc[mask, col] = np.nan` -- replace matching values with NaN\n",
        "\n",
        "**Expected result:** Approximately 8 negative values and 6 values over 100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.1: Check min/max of percentage columns\n",
        "\n",
        "Use `describe()` on the percentage columns and look at the `min` and `max` rows.\n",
        "Any `min` below 0 or `max` above 100 means there are invalid values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "percentage_cols = [\n",
        "    'desercion', 'desercion_primaria', 'desercion_secundaria', 'desercion_media',\n",
        "    'cobertura_neta', 'cobertura_neta_primaria', 'cobertura_neta_secundaria', 'cobertura_neta_media',\n",
        "    'aprobacion', 'reprobacion',\n",
        "]\n",
        "\n",
        "# YOUR CODE: show describe() for only the min and max rows\n",
        "# Hint: df[percentage_cols].describe().loc[['min', 'max']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.2: Count the invalid values\n",
        "\n",
        "Count exactly how many values are below 0 and above 100 in each column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: count negatives (values < 0) per column\n",
        "# Hint: df[percentage_cols].lt(0).sum()\n",
        "negatives = # YOUR CODE\n",
        "print(\"Negative values (< 0) per column:\")\n",
        "print(negatives[negatives > 0])\n",
        "print(f\"Total negatives: {negatives.sum()}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# YOUR CODE: count values > 100 per column\n",
        "# Hint: df[percentage_cols].gt(100).sum()\n",
        "over_100 = # YOUR CODE\n",
        "print(\"Values over 100 per column:\")\n",
        "print(over_100[over_100 > 0])\n",
        "print(f\"Total over 100: {over_100.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.3: Fix invalid values\n",
        "\n",
        "For each percentage column:\n",
        "1. Create a boolean mask: `(df[col] < 0) | (df[col] > 100)`\n",
        "2. Replace those values with NaN: `df.loc[mask, col] = np.nan`\n",
        "3. Fill NaN with the column median\n",
        "\n",
        "Loop through `percentage_cols` and apply this fix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_fixed = 0\n",
        "\n",
        "for col in percentage_cols:\n",
        "    # YOUR CODE: create mask for invalid values (< 0 or > 100)\n",
        "    invalid_mask = # YOUR CODE\n",
        "    n_invalid = invalid_mask.sum()\n",
        "    \n",
        "    if n_invalid > 0:\n",
        "        # YOUR CODE: replace invalid values with NaN\n",
        "        \n",
        "        # YOUR CODE: fill NaN with median\n",
        "        median_val = df[col].median()\n",
        "        \n",
        "        print(f\"{col}: fixed {n_invalid} invalid values (replaced with median {median_val:.2f})\")\n",
        "        total_fixed += n_invalid\n",
        "\n",
        "print(f\"\\nTotal invalid values fixed: {total_fixed}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.4: Verify the fix\n",
        "\n",
        "Confirm that all percentage columns now have values between 0 and 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE: show min and max for percentage_cols after fix\n",
        "# Verify: no min < 0 and no max > 100\n",
        "print(\"After fix -- Min and Max:\")\n",
        "# YOUR CODE\n",
        "\n",
        "\n",
        "# Count remaining invalid values\n",
        "remaining_invalid = (df[percentage_cols].lt(0).sum().sum() + df[percentage_cols].gt(100).sum().sum())\n",
        "print(f\"\\nInvalid values remaining: {remaining_invalid}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Documentation: Invalid Values\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "**1. How many negative values and how many values over 100 did you find?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**2. Why is domain knowledge necessary to catch these errors? Could an automated tool find them?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "**3. An alternative approach is \"clipping\": set negatives to 0 and values >100 to 100. When would clipping be better than replacing with the median? When would it be worse?**\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7: Final Verification\n",
        "\n",
        "Compare the original dataset with your cleaned version to see the full impact of your work.\n",
        "This cell is pre-filled. Run it and review the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 55)\n",
        "print(\"  BEFORE CLEANING (original)\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"  Rows:                {len(df_original)}\")\n",
        "print(f\"  Total NaN:           {df_original.isnull().sum().sum()}\")\n",
        "print(f\"  Duplicates:          {df_original.duplicated().sum()}\")\n",
        "print(f\"  Unique departments:  {df_original['departamento'].nunique()}\")\n",
        "print(f\"  ano dtype:           {df_original['ano'].dtype}\")\n",
        "print(f\"  poblacion dtype:     {df_original['poblacion_5_16'].dtype}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"  AFTER CLEANING\")\n",
        "print(\"=\" * 55)\n",
        "print(f\"  Rows:                {len(df)}\")\n",
        "print(f\"  Total NaN:           {df.isnull().sum().sum()}\")\n",
        "print(f\"  Duplicates:          {df.duplicated().sum()}\")\n",
        "print(f\"  Unique departments:  {df['departamento'].nunique()}\")\n",
        "print(f\"  ano dtype:           {df['ano'].dtype}\")\n",
        "print(f\"  poblacion dtype:     {df['poblacion_5_16'].dtype}\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"=\" * 55)\n",
        "print(\"  PERCENTAGE COLUMNS RANGE CHECK\")\n",
        "print(\"=\" * 55)\n",
        "pct_check = ['desercion', 'cobertura_neta', 'aprobacion', 'reprobacion']\n",
        "for col in pct_check:\n",
        "    print(f\"  {col}: min={df[col].min():.2f}, max={df[col].max():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verification Checklist\n",
        "\n",
        "Before submitting, verify each item. Change `[ ]` to `[x]` for each one you confirm:\n",
        "\n",
        "- [ ] **Missing values:** Zero NaN remaining (or justified remaining)\n",
        "- [ ] **Data types:** `ano` is `int64`, `poblacion_5_16` is `int64`\n",
        "- [ ] **Duplicates:** Zero duplicate rows remaining\n",
        "- [ ] **Text:** ~34 unique departments (all uppercase, no extra spaces)\n",
        "- [ ] **Invalid values:** All percentage columns between 0 and 100\n",
        "- [ ] **Row count:** ~462 rows (original 482 minus duplicates and dropped rows)\n",
        "- [ ] **All documentation cells filled in** with your reasoning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 8: Reflection\n",
        "\n",
        "Answer each question thoughtfully. These are graded.\n",
        "\n",
        "### 1. What was the most challenging data quality issue to fix? Why?\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "### 2. Which cleaning decisions required domain expertise about education data?\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "### 3. Data cleaning is iterative: fixing one problem can create another. Where did you experience this in the workshop?\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "### 4. How will you apply these steps to your project dataset from datos.gov.co?\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "### 5. If you had to clean this dataset again from scratch, what would you do differently?\n",
        "\n",
        "*YOUR ANSWER*\n",
        "\n",
        "---\n",
        "\n",
        "*Week 3 Workshop -- Data Analytics Course -- Universidad Cooperativa de Colombia*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
